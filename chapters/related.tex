Much work has been done in the area of alias analysis, both in proposing new alias analysis techniques, and for evaluating such techniques. Because alias analysis is an NP-Hard problems, many potential avenues exist for developing more precise or efficient approximations.

1997 Report Hind

\section{Evaluating Alias Analyses}
The first nearest analogue to this thesis's work can be found in Michael Hind and Anthony Pioli's research report, which attempts to measure several different alias analysis techniques under the same conditions and performance metrics. Specifically, Hind and Pioli explore three different techniques with varying degrees of precision and efficiency: Flow Insensitive Analysis, Flow Sensitive Analysis, and Flow Insensitive Analysis with Kill Information. These techniques are performed on input programs that are broken down into Control Flow Graphs (CFGs), and sets of pointer aliases are calculated at varying degrees of granularity. The Flow Insensitive analysis calculates possible aliases for variables across the entire function, with the Flow Insensitive Analysis with Kill including additional information about pointer definition and usage intended to improve the Flow Insensitive Analysis. On the other hand, the Flow Sensitive Analysis creates two alias sets for each CFG node, reflecting possible changes in the program due to control flow constructs. All three alias analyses are run on fourteen benchmark C programs, and precision is defined as the number of possible objects, or values, that a given pointer could refer to. Additional statistics are also collected from running these benchmarks, including the execution time of each analysis technique, distinctions between pointers used for reading or writing, and the type of pointer within the context of the program, such as local variables, global variables, formal parameters, and heap variables.

After running the benchmarks, the authors found that additional kill information did not improve the precision of Flow Insensitive analysis. The authors also found that the Flow Insensitive analysis was at least as precise, if not more so, than the Flow Sensitive analysis in half of the benchmarks used. The authors attribute this discrepancy to three possible causes: the first is that Flow Sensitive analysis becomes less precise as the size of a CFG increases, the second is that the consideration of formal and actual parameters is the same for Flow-Sensitive and Flow-Insenstive analyses, and the third is that pointers are often not modified in ways that would require the additional overhead needed for Flow-Sensitive analysis. The authors also propose efficiency improvements for alias analysis techniques, namely sharing alias sets between CFG nodes using Sparse Evaluation Graph (SEG) nodes to save space and reduce overhead in traversing the CFG, using sorted worklists to traverse CFG nodes, and only propagating alias relations that can be reached from a given function call. All of these improvements were shown to speed up the alias analyses in varying degrees. While this report does provide a detailed method of evaluating alias analysis techniques, its definition of precision is limited by the static nature of alias analysis techniques. Thus, there is no additional confirmation on whether a given alias is accurate with respect to the actual program.

2001 Report Hind

Hind and Pioli produced another report evaluating various alias analysis techniques that expands on their previous work \cite{Hind}. This time, they examined six different context-insensitive analysis techniques. Four of these are flow-insensitive, one is flow-sensitive, and one is flow-insensitive but with additional kill information. The first technique, Address Taken (AT) analysis, is flow insensitive and computes a single global alias set for all objects in the program that were assigned to another variable. With its linear time complexity and limited precision, AT served as a baseline technique for comparison with the other techniques. The next technique, Steensgaard (ST) analysis, is a flow-insensitive analysis that computes a single union/find alias set in a single pass in almost linear time. The next flow-insensitive technique, Andersen (AN) analysis, implements Andersen's algorithm; normally, this algorithm uses constraint solving, but in the interest of efficiency, the analysis technique used in this report uses an iterative dataflow. Two flow-insensitive techniques proposed by Burke et al (B1 and B2) calculate local alias sets for each function call, with B2 including additional kill information for variable definition and usage. The final flow-sensitive algorithm proposed by Choi et al (CH) operates similarly to the B1 technique, but at the level of SEG nodes insteads of at the function level.

For this report, Hind and Pioli used twenty-four benchmark C programs, varying from under 1000 to almost 30,000 lines of code. These benchmarks themselves are measured through the resulting CFG's that are created for the pointer analyses, such as the number of CFG nodes, the number of function calls, and the number of heap allocations. As with the previous report, precision for each analysis technique is defined by the number of possible aliases for each given pointer. In addition to precision, execution time, memory usage, and the number of pointer reads and writes are measured for each analysis technique.

After running the benchmarks, both AT and ST were found to be efficient in terms of speed and memory usage. ST was significantly more precise than AT, especially as programs increased in size, with only minimal increases in overhead. AN and B1 varied in comparison to each other, with one significantly outperforming the other, and vice versa, in different benchmarks. The B2 analysis was consistently slower than B1, and the CH analysis was generally significantly slower, save for some benchmarks. The AN, B1, and B2 analyses had the same level of precision as one another, and was comparable with the CH analysis for many of the benchmarks. Thus, the additional kill information in B2 was again, not found to provide any significant benefit in increasing precision. As with the previous report, the benchmarks did not encounter the types of statements that would benefit from flow-sensitive analysis. Additionally, the CH analysis's memory usage was several times higher than that of its flow-insensitive counterparts, even after additional optimizations used to reduce its memory footprint. The speed of pointer analysis was found to be dependent on both program size and the number of propagated alias relations throughout a program's graph. Because this report is an expansion on a previous experiment, it also possesses the same limitations as the previous experiments, namely the definition of precision being limited by purely static analysis techniques.

\section{Proposed Analysis Techniques}

Because much of the overhead behind inclusion-based pointer analysis is due to the size of the generated constraint graph describing the relationship between pointer aliases, Hardekopf and Lin proposed two new algorithms to detect cycles within constraint graphs to reduce the size of the graph \cite{Hardekopf}. The first method, Lazy Cycle Detection (LCD), occurs when an alias set is propagated across nodes in the constraint graph; LCD checks the constraint graph for cycles based on two conditions; the first is whether or not two alias sets are identical, and the second is whether or not the graph edge related to the current pointer relation was searched previously. The second method, Hybrid Cycle Detection (HCD) performs a static analysis of the program before the actual pointer analysis to create a constraint graph and collapse any possible cycles; this preprocessing reduces the number of traverals performed by the actual pointer analysis, and provides additional information about which pointers might be part of a cycle, even if its alias sets are incomplete after performing HCD. LCD and HCD are evaluated with other comparative optimization algorithms for inclusion-based pointer analysis in five C benchmarks. In addition to the overall reduced number of constraints for each benchmark, the execution time and memory usage is measured for each algorithm. For the benchmarks, HCD is measured both by itself and in combination with the other algorithms. As individual algorithms, LCD and HCD had comparable execution times with the other algorithms. However, while LCD's memory usage was on par with the other algorithms, due to its preprocessing nature, HCD by itself could not complete all of the benchmarks due to running out of memory. When used in tandem, LCD and HCD significantly outperformed the other algorithms in terms of speed, with minimal decreases in memory usage. HCD also provided similar performance improvements when used in conjunction with the other algorithms.

One of the alias analysis techniques used in Hardekopf and Lin's experiments to compare against their proposed algorithms was a context-insensitive pointer analysis method developed by Pearce et al to account for fields and function pointers in an efficient, precise manner \cite{Pearce}. Previously, context-insensitive pointer analyses lacked the constraint types necessary to accurately reflect references to fields within user-defined structures; aggregate types were generally treated as a single variable, or treated as a distinct set of fields for either a unique instance of an aggregate or for all aggregates of the same type. Additionally, function pointers lacked any particular notation that could be used in an elegant or efficient manner. To account for these shortcomings, the authors introduced pointer constraints that included integer offsets, along with inference rules that utilize these constraints. These offsets can be used to model aggregate fields, and functions based on their addresses and parameters, and are treated as edge weights in a constraint graph. After running both field-sensitive and field-insensitive versions of their new analysis technique on seven benchmarks, Pearce et al found that field-sensitive analysis offered more precision, but at the cost of increased execution time. However, the increase in precision was also found to decrease with larger programs.
