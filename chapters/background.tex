\section{Control Flow Graphs}
When compilers convert a source language to the underlying machine code, they first organize the program's statements into a form that is useful for subsequent operations. The compiler constructs a Control Flow Graph (CFG) for each function that separates groups of statements based on the language's available control flow constructs, such as conditional statements or loops. Statements are grouped into basic blocks, and are connected to one another based on their corresponding control flow statements; larger blocks are encouraged to provide more opportunities for program optimizations. After each graph is generated, the compiler converts the statements from each block into the appropriate machine code and outputs each converted block. Control Flow Graphs can also keep track of other information that is useful for later optimizations, such as each basic block's predecessors.

\section{Intermediate Representations}
Some compilers use an Intermediate Representation (IR) for the source language before converting the input program to the appropriate machine code. The IR provides additional information, such as data types, at a lower abstraction level than the input language, and can be processed more easily than the final machine code. Optimizations are often performed after the program is converted to an IR due to having more opportunities to optimize at this level without having to address platform-specific details.

\subsection{LLVM}
The Low Level Virtual Machine (LLVM) IR is commonly used for compiler construction. This IR features instructions similar to those of assembly languages, but also includes features available in higher-level languages. Additional abstractions provided by LLVM include virtual registers, register and variable types, and function headers and calls, removing the overhead needed to maintain calling conventions. Each virtual register in the LLVM IR is unique and can only declared once, a convention known as Single Static Assignment (SSA). LLVM's virtual registers are later mapped to real registers when the program is converted to binary code.

\section{Optimizations}
After creating the CFG for the input program and producing the corresponding IR, a compiler may take one or more optimization passes on the graph. These optimizations are meant to improve program performance without affecting the semantic meaning of the program, and focus on reducing unnecessary code, execution time, and memory usage.

\subsection{SSA Optimization}
Because loading and storing variables from memory can incur time overhead, some compilers minimize the use of memory by storing variables exclusively within registers. This is effective with an IR that enforces SSA because whenever a value is updated, including ones from variables, that value must be assigned to a new virtual register. Optimization using SSA requires recursively searching through a basic block's predecessors to find the last register that contained a desired value. Additionally, the value of a variable may exist in different registers depending on the path of a program, so additional overhead is required to unify different versions of a variable from its predecessors at the beginning of a basic block. Despite the increase in instructions, the reduced time from accessing registers instead of accessing main memory provides significant improvements in speed and memory usage at the cost of larger executable sizes.

\subsection{Constant Propagation}
Certain constants may be known at compilation time within a program. A compiler can replace operands within statements and expressions with known constants, potentially collapsing multiple expressions into single values. Conditions that are replaced with constants may change the structure of the CFG by removing basic blocks that are never traversed.

\subsection{Code Removal and Relocation}
Instructions that do not affect other instructions or have no effect on the program can be removed. Instructions that produce the same result within loops or conditional statements may be relocated to surrounding basic blocks to reduce the amount of redundant calculation. Certain instructions, such as load and store instructions, may be relocated to improve performance based on hardware-based considerations, such as pipelining. When relocating instructions, additional analysis is required to ensure that these instructions do not have additional dependencies from nearby instructions, either in the form of operands in later instructions or by updating required variables or values. One such analysis that determines whether certain values are related is Alias Analysis.

\section{Alias Analysis}
Two pointers are said to alias if they refer to the same area of program memory. An alias analysis attempts to determine which pointers in a program alias with each other. Because alias analysis is an NP-Hard problem, conventional alias analysis techniques perform some kind of approximation when producing sets of possible aliases. Because the analysis is imperfect, compilers must make conservative assumptions when performing optimizations based on the results of an alias analysis.

\subsection{Andersen Analysis}
Andersen Alias Analysis is a analysis technique for determining pointer aliases within functions without considering program flow. Andersen analysis provides set notation and type inference rules meant for the C programming language. Pointers are initially stated to be part of specific types of pointers, such as global variables, dynamically allocated memory, and function parameters. Additional type inference rules are used to represent different operations performed with pointers, such as dereferencing, assignment, and type casting. These rules are used to generate set constraints for the pointer values within a function. For alias analysis across function calls, Andersen analysis uses static call graphs and additional inference rules to generate context-sensitive constraints. The generated constraints for the aliases are solved by a using set of rewriting rules for type normalization and propagation; the constraint solving algorithm's time complexity is polynomial in terms of program size.

\subsection{Steensgaard Analysis}
Steensgaard Alias Analysis is another alias analysis technique that works across function calls without considering program flow. Steensgaard analysis is based off an abstract pointer-based language that includes pointer operations, n-ary operators, dynamic memory, and functions. Type inference rules are used to generate alias sets for the pointer variables in the program. Each statement is initially processed once to generate the initial set of pointers; these values are stored in a union-find data structure, and are combined in alias sets in subsequent join operations. The resulting algorithm was found to be linear in space complexity and almost linear in terms of time complexity.
