\chapter{Background}

\section{Control Flow Graphs}
When compilers convert a source language to the underlying machine code, they first organize the program's statements into a form that is useful for subsequent operations. The compiler constructs a Control Flow Graph (CFG) for each function that separates groups of statements based on the language's available control flow constructs, such as conditional statements or loops. Statements are grouped into basic blocks, and are connected to one another based on their corresponding control flow statements; larger blocks are encouraged to provide more opportunities for program optimizations. After each graph is generated, the compiler converts the statements from each block into the appropriate machine code and outputs each converted block.

\section{Intermediate Representations}
Some compilers use an Intermediate Representation (IR) for the source language before converting the input program to the appropriate machine code. The IR provides additional information, such as data types, at a lower abstraction level than the input language, and can be processed more easily than the final machine code. Optimizations are often performed after the program is converted to an IR due to having more opportunities to optimize at this level without having to address platform-specific details.

\subsection{LLVM}
The Low Level Virtual Machine (LLVM) IR is commonly used for compiler construction. This IR features instructions similar to those of assembly languages, but also includes features available in higher-level languages. Additional abstractions provided by LLVM include virtual registers, register and value types, and function headers and calls, removing the overhead needed to maintain calling conventions. Each virtual register in the LLVM IR is unique and can only be defined once, a convention known as Single Static Assignment (SSA). LLVM's virtual registers are later mapped to real registers when the program is converted to binary code.

\section{Optimizations}
After creating the CFG for the input program and producing the corresponding IR, a compiler may take one or more optimization passes on the graph. These optimizations are meant to improve program performance without affecting the semantics of the program, and focus on reducing unnecessary code, execution time, and memory usage. CFG's can also keep track of other information that is useful for later optimizations, such as each basic block's predecessors.

\subsection{SSA Optimization}
Because loading and storing variables from memory can incur time overhead, some compilers minimize the use of memory by storing variables exclusively within registers. This is effective with an IR that enforces SSA because whenever a value is updated, including ones from variables, that value must be assigned to a new virtual register. Optimizing programs to follow SSA form requires recursively searching through a basic block's predecessors to find the last register that contained a desired value. %Additionally, the value of a variable may exist in different registers depending on the path of a program, so additional overhead is required to unify different versions of a variable from its predecessors at the beginning of a basic block. Despite the increase in instructions, the reduced time from accessing registers instead of accessing main memory provides significant improvements in speed and memory usage at the cost of larger executable sizes.

\subsection{Constant Propagation}
Certain constants may be known at compilation time within a program. A compiler can replace operands within statements and expressions with known constants, potentially collapsing multiple expressions into single values. Conditions that are replaced with constants may change the structure of the CFG by removing basic blocks that are never traversed. By simplifying the structure of the CFG without changing the program's meaning, this optimization reduces potential ambiguities caused by unnecessary branches, which is useful for code generation and subsequent program analyses and optimizations.

\subsection{Code Removal and Relocation}
Instructions that do not affect other instructions or have no effect on the program can be removed. Instructions that produce the same result within loops or conditional statements may be relocated to surrounding basic blocks to reduce the amount of redundant calculation. Certain instructions, such as load and store instructions, may be relocated to improve performance based on hardware-based considerations, such as pipelining. When relocating instructions, additional analysis is required to ensure that these instructions do not have additional dependencies from nearby instructions, either in the form of operands in later instructions or by updating required variables or values. One such analysis that determines whether certain values are related is Alias Analysis.

\section{Alias Analysis}
Two pointers are said to alias if they refer to the same area of program memory. An alias analysis attempts to determine which pointers in a program are aliases. Because alias analysis is an undecidable problem \cite{undecidable}, conventional alias analysis techniques perform some kind of approximation when producing sets of possible aliases. Because the analysis is imperfect, compilers must make conservative assumptions when performing optimizations based on the results of an alias analysis. Alias analyses vary in terms of how effective they can examine programs; the deeper a program can analyze, the more complex it is. Some analyses are intraprocedural, and are limited to analyzing single functions, while other analyses are interprocedural and can analyze entire programs. An alias analysis is flow-sensitive if it accounts for changes in aliasing caused by program flow, such as conditional statements or loops, and is context-sensitive if it accounts for aliases that exist between function calls.
