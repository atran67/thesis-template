To gather data for alias analyses, we instrument a series of benchmark C programs. These programs vary in size and complexity, and are designed to represent various realistic workloads. The C source files are converted to the LLVM IR and compiled using clang, without optimizations. Because the instrumentation requires additional I/O and time overhead, the benchmarks are run either until completion or after 10 minutes have passed. The data from the benchmarks is used with the results from several implemented alias analyses to measure the effectiveness of those techniques.

\section{SPEC2000 benchmarks}
Seven of the benchmarks used to gather data are from the SPEC2000 benchmark suite. These benchmarks are designed to test a platform's various CPU, memory, and I/O capabilities, and consist of multiple C source files, and reference input data with expected outputs. The original benchmarks were compiled with GCC using the -O3 optimization flag, which was omitted when converting their respective source files to the llVM IR. Because of the large amount of code in these benchmarks, we only instrument functions with two or more pointer parameters. We use this requirement to examine functions where we are more likely to see meaningful aliasing, and to reduce the amount of instrumentation data that is not meaningful to evaluating alias analyses; we expect functions that take in multiple pointers to reference and utilize them in more elaborate ways, potentially resulting in more aliases.

\section{PLB benchmarks}
We also instrument three benchmarks from the Programming Language Benchmark (PLB) Suite \cite{plb}. While these benchmarks are used to primarily measure performance differences between programming languages, we are interested in the C implementations of these benchmarks for gathering data about aliases. Specifically, we use the sudoku, matmul, and dict benchmarks from the PLB suite. Due to the two-dimensional nature of Sudoku puzzles and matrix multiplication, we expect meaningful amounts of aliasing to occur within the program. We also expect similar amounts of aliasing for the dict benchmark due to the nature of creating, updating, and traversing dictionaries. In terms of code size, the PLB benchmarks are smaller, consisting of single source files for each benchmark, and are expected to complete before 10 minutes have elapsed.
