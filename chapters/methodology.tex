\chapter{Methodology}

To gather data for alias analyses, we instrument a series of benchmark C programs. These programs vary in size and complexity, and are designed to represent various realistic workloads. The C source files are converted to the LLVM IR and compiled using clang, without optimizations. Because the instrumentation requires additional I/O and time overhead, the benchmarks are run either until completion or after 10 minutes have passed. The data from the benchmarks is used with the results from several implemented alias analyses to measure the effectiveness of those techniques.

\section{SPEC2000 Benchmarks}
Seven of the benchmarks used to gather data are from the SPEC2000 benchmark suite. These benchmarks are designed to test a platform's various CPU, memory, and I/O capabilities, and consist of multiple C source files, and reference input data with expected outputs. The original benchmarks were compiled with GCC using the -O3 optimization flag, which was omitted when converting their respective source files to the llVM IR. Because of the large amount of code in these benchmarks, we only instrument functions with two or more pointer parameters. We use this requirement to examine functions where we are more likely to see meaningful aliasing, and to reduce the amount of instrumentation data that is not meaningful to evaluating alias analyses; we expect functions that take in multiple pointers to reference and utilize them in more elaborate ways, potentially resulting in more aliases.

\section{PLB Benchmarks}
We also instrument three benchmarks from the Programming Language Benchmark (PLB) Suite \cite{plb}. While these benchmarks are used to primarily measure performance differences between programming languages, we are interested in the C implementations of these benchmarks for gathering data about aliases. Specifically, we use the sudoku, matmul, and dict benchmarks from the PLB suite. Due to the two-dimensional nature of Sudoku puzzles and matrix multiplication, we expect meaningful amounts of aliasing to occur within the program. We also expect similar amounts of aliasing for the dict benchmark due to the nature of creating, updating, and traversing dictionaries. In terms of code size, the PLB benchmarks are smaller, consisting of single source files for each benchmark, and are expected to complete before 10 minutes have elapsed. Unlike the SPEC2000 benchmarks, all functions in these benchmarks are instrumented.

\section{Other Benchmarks}
We include three benchmarks that are meant to test implementations of the standard library malloc function \cite{malloc}. While the function calls differ, each of the benchmarks repeatedly call malloc and free, making them ideal for studying allocation sizes and lifetimes. These benchmarks are also small, so all functions for each benchmark are instrumented. Two small customized benchmarks were also written for testing alias analyses, and are instrumented at the same level as the other smaller benchmarks; one is a search tree that stores words based on common prefixes, while the other searches for cycles within a linked list at different rates. 

\section{Alias Analysis}
We gather alias data from four alias analyses implemented by the LLVM optimizer, consisting of the Andersen, Steensgaard, Objective-C based Automatic Reference Counting (ARC), and Basic Alias Analyses. The optimizer outputs information for each LLVM file in the form of alias sets, each containing virtual registers or variables that must, or may alias with each other.
